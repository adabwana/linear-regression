[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "(ns index)\n\n\n1 Assignment 1: Linear Regression\nInstructions:\n\nSelect a suitable dataset from the UCI Machine Learning Repository\nScale, normalize, and/or encode your data appropriately.\nImplement the Multivariate Linear Regression.\nCompare the performance of your implementation in terms of computation time and other appropriate metrics using 5-fold Cross Validation using a variety of Alpha values.\nModify the “README.md” file to include the following sections:     * Summary: A one-paragraph summary of the algorithm that was implemented including any pertinent or useful information. If mathematics are appropriate, include those as well.     * Results: Display the comparative analysis of your implementation and the benchmark implementation in terms of computation time and other metrics. Which algorithm performed better? Why do you think this is the case?     * Reflection: One paragraph describing useful takeaways from the week, things that surprised you, or things that caused you inordinate difficulty.\nMake sure that your README file is formatted properly and is visually appealing. It should be free of grammatical errors, punctuation errors, capitalization issues, etc.\n\nWhat I did:\n\nSelected Liver Disorders. Interesting dataset that has been misunderstood in most publications.\nScaled all regressors and removed an indicator column called :selector.\nImplemented three popular regularization techniques–ridge, lasso, and elastic net regression.\nCompared Clojure’s native Scicloj machine learning library that leverages Java’s Smile machine learning library (no alpha parameter, so I used lambda1 and lambda2) against Clojure’s implementation of Python’s Scikit Learn library (does have alpha values I configured) using sklearn-clj library.\n\n\n\n\n\nsource: src/index.clj"
  },
  {
    "objectID": "assignment.eda.html#tables-and-nature-of-data",
    "href": "assignment.eda.html#tables-and-nature-of-data",
    "title": "2  Exploratory Data Analysis",
    "section": "2.1 Tables and nature of data",
    "text": "2.1 Tables and nature of data\nFirst seven of data\n\n(ds/head liver-disease 7)\n\ndata/bupa.csv [7 7]:\n\n\n\n:mcv\n:alkphos\n:sgpt\n:sgot\n:gammagt\n:drinks\n:selector\n\n\n\n\n85\n92\n45\n27\n31\n0.0\n1\n\n\n85\n64\n59\n32\n23\n0.0\n2\n\n\n86\n54\n33\n16\n54\n0.0\n2\n\n\n91\n78\n34\n24\n36\n0.0\n2\n\n\n87\n70\n12\n28\n10\n0.0\n2\n\n\n98\n55\n13\n17\n17\n0.0\n2\n\n\n88\n62\n20\n17\n9\n0.5\n1\n\n\n\nDescriptive statistics of columns\n\n(ds/info liver-disease)\n\ndata/bupa.csv: descriptive-stats [7 11]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:col-name\n:datatype\n:n-valid\n:n-missing\n:min\n:mean\n:max\n:standard-deviation\n:skew\n:first\n:last\n\n\n\n\n:mcv\n:int16\n345\n0\n65.0\n90.15942029\n103.0\n4.44809597\n-0.38843312\n85.0\n98.0\n\n\n:alkphos\n:int16\n345\n0\n23.0\n69.86956522\n138.0\n18.34767034\n0.75366677\n92.0\n99.0\n\n\n:sgpt\n:int16\n345\n0\n4.0\n30.40579710\n155.0\n19.51230891\n3.06349864\n45.0\n57.0\n\n\n:sgot\n:int16\n345\n0\n5.0\n24.64347826\n82.0\n10.06449375\n2.29307245\n27.0\n45.0\n\n\n:gammagt\n:int16\n345\n0\n5.0\n38.28405797\n297.0\n39.25461617\n2.86609356\n31.0\n65.0\n\n\n:drinks\n:float64\n345\n0\n0.0\n3.45507246\n20.0\n3.33783526\n1.54381943\n0.0\n20.0\n\n\n:selector\n:int16\n345\n0\n1.0\n1.57971014\n2.0\n0.49432233\n-0.32438319\n1.0\n1.0\n\n\n\n\n2.1.1 What is column :selector?\n\n(set (:selector liver-disease))\n\n\n#{1 2}\n\n:selector takes two value only, 0 and 1. Let’s see descriptive statistics of each group.\n\n(ds/info (tech.v3.dataset/filter liver-disease #(= (:selector %) 1)))\n\ndata/bupa.csv: descriptive-stats [7 11]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:col-name\n:datatype\n:n-valid\n:n-missing\n:min\n:mean\n:max\n:standard-deviation\n:skew\n:first\n:last\n\n\n\n\n:mcv\n:int16\n145\n0\n78.0\n90.63448276\n99.0\n3.86906139\n-0.13253898\n85.0\n98.0\n\n\n:alkphos\n:int16\n145\n0\n23.0\n71.97931034\n138.0\n18.59079408\n0.62377127\n92.0\n99.0\n\n\n:sgpt\n:int16\n145\n0\n10.0\n31.20689655\n103.0\n15.77792786\n1.88282131\n45.0\n57.0\n\n\n:sgot\n:int16\n145\n0\n5.0\n22.78620690\n57.0\n7.73806088\n1.14900370\n27.0\n45.0\n\n\n:gammagt\n:int16\n145\n0\n5.0\n31.54482759\n203.0\n33.22502807\n2.83802686\n31.0\n65.0\n\n\n:drinks\n:float64\n145\n0\n0.0\n3.54137931\n20.0\n3.92972268\n1.80063118\n0.0\n20.0\n\n\n:selector\n:int16\n145\n0\n1.0\n1.00000000\n1.0\n0.00000000\n\n1.0\n1.0\n\n\n\n\n(ds/info (tech.v3.dataset/filter liver-disease #(= (:selector %) 2)))\n\ndata/bupa.csv: descriptive-stats [7 11]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:col-name\n:datatype\n:n-valid\n:n-missing\n:min\n:mean\n:max\n:standard-deviation\n:skew\n:first\n:last\n\n\n\n\n:mcv\n:int16\n200\n0\n65.0\n89.8150\n103.0\n4.80481072\n-0.40881586\n85.0\n96.0\n\n\n:alkphos\n:int16\n200\n0\n37.0\n68.3400\n134.0\n18.06199263\n0.86583251\n64.0\n69.0\n\n\n:sgpt\n:int16\n200\n0\n4.0\n29.8250\n155.0\n21.84491650\n3.31719988\n59.0\n53.0\n\n\n:sgot\n:int16\n200\n0\n8.0\n25.9900\n82.0\n11.28880354\n2.36380090\n32.0\n43.0\n\n\n:gammagt\n:int16\n200\n0\n8.0\n43.1700\n297.0\n42.51846894\n2.80462310\n23.0\n203.0\n\n\n:drinks\n:float64\n200\n0\n0.0\n3.3925\n12.0\n2.84166661\n0.84994446\n0.0\n12.0\n\n\n:selector\n:int16\n200\n0\n2.0\n2.0000\n2.0\n0.00000000\n\n2.0\n2.0\n\n\n\nThe data looks similar in terms of summary statistics per column in either selector equals 1 or 2, above. Three columns I am focusing on are, mean, standard deviation, and skew. Mean, standard deviation, and skew are similar for both groups in :mcv and :alkphos. The other four columns deviate in some way from the other :selector groups in either mean, standard deviation, and/or skew."
  },
  {
    "objectID": "assignment.eda.html#plots",
    "href": "assignment.eda.html#plots",
    "title": "2  Exploratory Data Analysis",
    "section": "2.2 Plots",
    "text": "2.2 Plots\n\n(def cols-of-interest\n  (remove #{:selector} (ds/column-names liver-disease)))\n\n\n2.2.1 Linearity with response and histogram of regressor.\n\n^kind/vega\n(let [dataset liver-disease\n      make-plot (fn [field]\n                  (-&gt; dataset\n                      (hanami/vconcat {}                    ;can switch to hconcat\n                                      [(hanami/plot dataset ht/point-chart\n                                                    {:X field :XSCALE {:zero false}\n                                                     :Y :drinks :YSCALE {:zero false}})\n                                       (hanami/histogram dataset field {:nbins 20})])))]\n  (-&gt;&gt; (map make-plot cols-of-interest) (hanami/hconcat {} {})))\n\n\n\n\n\n\n\n(comment                                                    ;make plot alternative were I can change height and width but not nbins\n  (let [dataset liver-disease]\n    (-&gt; dataset\n        (hanami/hconcat {}\n                        [(hanami/plot dataset ht/point-chart\n                                      {:HEIGHT 125 :WIDTH 175\n                                       :X      :mcv :XSCALE {:zero false}\n                                       :Y      :drinks :YSCALE {:zero false}})\n                         (hanami/plot dataset ht/bar-chart\n                                      {:HEIGHT 125 :WIDTH 175\n                                       :X      field :YAGG :count})]))))\n\n\n\n2.2.2 Pairs-plots.\n\n^kind/vega\n(let [data (ds/rows liver-disease :as-maps)\n      column-names cols-of-interest]\n  {:data   {:values data}\n   :repeat {:column column-names\n            :row    column-names}\n   :spec   {:height   100 :width 100\n            :mark     \"circle\"\n            :encoding {:x {:field {:repeat \"column\"} :type \"quantitative\" :scale {:zero false}}\n                       :y {:field {:repeat \"row\"} :type \"quantitative\" :scale {:zero false}}}}})\n\n\n\n\n\n\n\n(let [combos (combo/combinations cols-of-interest 2)]\n  (for [[x y] combos]\n    (assoc {} [x y] (stats/correlation (get liver-disease x) (get liver-disease y)))))\n\n\n({[:mcv :alkphos] 0.04410299969572895}\n {[:mcv :sgpt] 0.14769504803185451}\n {[:mcv :sgot] 0.1877651531698029}\n {[:mcv :gammagt] 0.2223144936927525}\n {[:mcv :drinks] 0.3126795966511246}\n {[:alkphos :sgpt] 0.07620760528013328}\n {[:alkphos :sgot] 0.1460565465048546}\n {[:alkphos :gammagt] 0.13314039838436367}\n {[:alkphos :drinks] 0.10079605839698665}\n {[:sgpt :sgot] 0.7396748677149187}\n {[:sgpt :gammagt] 0.5034352544029691}\n {[:sgpt :drinks] 0.20684793492452233}\n {[:sgot :gammagt] 0.5276259280455446}\n {[:sgot :drinks] 0.2795877736826015}\n {[:gammagt :drinks] 0.3412239586693896})\n\nWe can see pearson correlation between each pair of variables of interest. Correlations closer to |1| represent variables that have a strong relationship to each other. For example, Looking at both the pairs-plot and the correlations, we see :sgpt :sgot are the most related variables and are related in the positive direction. Below we focus on the correlation of regressors on the response only.\n\n(let [combos (combo/combinations cols-of-interest 2)]\n  (for [[x y] combos\n        :when (or (= :drinks x) (= :drinks y))]\n    (assoc {} [x y] (stats/correlation (get liver-disease x) (get liver-disease y)))))\n\n\n({[:mcv :drinks] 0.3126795966511246}\n {[:alkphos :drinks] 0.10079605839698665}\n {[:sgpt :drinks] 0.20684793492452233}\n {[:sgot :drinks] 0.2795877736826015}\n {[:gammagt :drinks] 0.3412239586693896})\n\n\n(comment                                                    ;another way to make histograms for diagonals\n  (let [data (ds/rows liver-disease :as-maps)\n        column-names cols-of-interest]\n    {:data   {:values data}\n     :repeat {:column column-names}\n     :spec   {:mark     \"bar\"\n              :encoding {:x {:field {:repeat \"column\"} :type \"quantitative\"}\n                         :y {:aggregate \"count\"}}}})\n\n  (stats/correlation-matrix (ds/columns liver-disease) :spearman))\n\n\n\n2.2.3 Box-plots.\n\n^kind/vega\n(let [data (ds/rows liver-disease :as-maps)\n      column-names (remove #{:selector} (ds/column-names liver-disease))]\n  {:data   {:values data}\n   :repeat {:column column-names}\n   :spec   {:width    60 :mark \"boxplot\"\n            :encoding {:y {:field {:repeat \"column\"} :type \"quantitative\" :scale {:zero false}}}}})\n\n\n\n\n\n\nLooking at the box-plots, the circle points are outliers, viz. points outside Q1 - 1.5IQR or Q3 + 1.5IQR. Below we count the number of outliers per column.\n\n(let [columns cols-of-interest]\n  (for [column columns]\n    (assoc {} column (count (stats/outliers (get liver-disease column))))))\n\n\n({:mcv 2} {:alkphos 9} {:sgpt 28} {:sgot 24} {:gammagt 27} {:drinks 5})\n\n\n(comment\n  ; created permutation of columns of interest. Wanted combinations, below\n  (for [x cols-of-interest\n        y (rest cols-of-interest)\n        :when (not= x y)]\n    [x y]))\n\n\n\n\n\nsource: src/assignment/eda.clj"
  },
  {
    "objectID": "assignment.scicloj.html#build-pipelines",
    "href": "assignment.scicloj.html#build-pipelines",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.1 Build pipelines",
    "text": "3.1 Build pipelines\n\n(def pipeline-fn\n  (ml/pipeline\n    (mm/remove-column :selector)\n    (mm/std-scale regressors {})\n    (mm/set-inference-target response)))\n\nI’m building three different pipelines because smile’s implementation of elastic net, the parameters are lambda1 and lambda2. I cannot make either of those parameters 0. If I do, the model throws and errors and says to try the explicit model. That is, if \\(lambda1 = 0\\), elastic net throws an error saying, “try using a ridge regression model.”\n\n(defn ridge-pipe-fn [params]                                ;alpha = 0\n  (ml/pipeline\n    pipeline-fn\n    {:metamorph/id :model}\n    (mm/model (merge {:model-type :smile.regression/ridge}\n                     params))))\n\n\n(defn lasso-pipe-fn [params]                                ;alpha = 1\n  (ml/pipeline\n    pipeline-fn\n    {:metamorph/id :model}\n    (mm/model (merge {:model-type :smile.regression/lasso}\n                     params))))\n\n\n(defn elastic-net-pipe-fn [params]                          ;0 &lt; alpha &lt; 1\n  (ml/pipeline\n    pipeline-fn\n    {:metamorph/id :model}\n    (mm/model (merge {:model-type :smile.regression/elastic-net}\n                     params))))"
  },
  {
    "objectID": "assignment.scicloj.html#partition-data",
    "href": "assignment.scicloj.html#partition-data",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.2 Partition data",
    "text": "3.2 Partition data\n\n(def ds-split\n  (ds/split-&gt;seq\n    liver-disease\n    :kfold {:seed 123 :k 5 :ratio [0.8 0.2]}))\n\n:split-names [:train-val :test]\n\n(def train-val-splits\n  (ds/split-&gt;seq\n    (:train (first ds-split))\n    :kfold {:seed 123 :k 5}))"
  },
  {
    "objectID": "assignment.scicloj.html#build-models",
    "href": "assignment.scicloj.html#build-models",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.3 Build models",
    "text": "3.3 Build models\n\n3.3.1 Ridge\n\n(def ridge-pipelines\n  (-&gt;&gt; (ml/sobol-gridsearch {:lambda (ml/linear 0 1000 250)})\n       (map ridge-pipe-fn)))\n\n\n(def eval-ridge-val\n  (evaluate-pipe ridge-pipelines train-val-splits))\n\n\n\n3.3.2 Lasso\n\n(def lasso-pipelines\n  (-&gt;&gt; (ml/sobol-gridsearch {:lambda (ml/linear 0 1000 250)})\n       (map lasso-pipe-fn)))\n\n\n(def eval-lasso-val\n  (evaluate-pipe lasso-pipelines train-val-splits))\n\n\n\n3.3.3 Elastic Net\n\n(def elastic-pipelines\n  (-&gt;&gt; (ml/sobol-gridsearch {:lambda1 (ml/linear 0.0001 100 16)\n                             :lambda2 (ml/linear 0.0001 100 16)})\n       (map elastic-net-pipe-fn)))\n\n\n(comment\n  (def elastic-pipelines\n    (-&gt;&gt; (ml/sobol-gridsearch\n           (dissoc\n             (ml/hyperparameters :smile.regression/elastic-net)\n             :tolerance :max-iterations))\n         (take 500)\n         (map elastic-net-pipe-fn))))\n\n\n(def eval-enet-val\n  (evaluate-pipe elastic-pipelines train-val-splits))"
  },
  {
    "objectID": "assignment.scicloj.html#extract-models",
    "href": "assignment.scicloj.html#extract-models",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.4 Extract models",
    "text": "3.4 Extract models\n\n(def models-ridge-val\n  (-&gt; (best-models eval-ridge-val)\n      reverse))\n\n\n(def models-lasso-val\n  (-&gt; (best-models eval-lasso-val)\n      reverse))\n\n\n(def models-enet-val\n  (-&gt; (best-models eval-enet-val)\n      reverse))\n\n\n3.4.1 Best model for each pipeline\n\n(-&gt; models-ridge-val first :summary)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -6.7453     -2.4532     -0.3884      1.6303     14.5707\n\nCoefficients:\nIntercept           3.5679\nmcv                 0.4123\nalkphos             0.1640\nsgpt                0.0857\nsgot                0.3015\ngammagt             0.3779\n\nResidual standard error: 3.1737 on 216 degrees of freedom\nMultiple R-squared: 0.1375,    Adjusted R-squared: 0.1216\nF-statistic: 8.6103 on 5 and 216 DF,  p-value: 1.823e-06\n\n\n(-&gt; models-lasso-val first :summary)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -3.2421     -2.7421     -1.2421      1.7579     16.7579\n\nCoefficients:\nIntercept           3.2421\nmcv                 0.0000\nalkphos             0.0000\nsgpt                0.0000\nsgot                0.0000\ngammagt             0.0000\n\nResidual standard error: 3.3179 on 216 degrees of freedom\nMultiple R-squared: 0.0000,    Adjusted R-squared: -0.0185\nF-statistic: 0.0000 on 5 and 216 DF,  p-value: 1.000\n\n\n(-&gt; models-enet-val first :summary)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -5.9860     -2.5215     -0.3551      1.5392     15.0635\n\nCoefficients:\nIntercept           3.5679\nmcv                 0.3809\nalkphos             0.0016\nsgpt                0.0002\nsgot                0.2526\ngammagt             0.3754\n\nResidual standard error: 3.1991 on 216 degrees of freedom\nMultiple R-squared: 0.1236,    Adjusted R-squared: 0.1074\nF-statistic: 7.6179 on 5 and 216 DF,  p-value: 9.270e-06\n\nThe summary for our best lasso model looks wrong. A negative Adjusted R\\(2\\)? We can see an issue derives from not calculating the Adjusted R\\(^2\\), i.e the :metric.\n\n(-&gt; models-lasso-val first :metric)\n\n\n##NaN\n\nInstead, we will collect the best lasso models according to the lowest mae, i.e. :other-metric-1.\n\n(def models-lasso-val-2\n  (-&gt;&gt; (best-models eval-lasso-val)\n       (sort-by :other-metric-1)))\n\n\n(-&gt; models-lasso-val-2 first :summary)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -7.5909     -2.2128     -0.4783      1.8028     13.6481\n\nCoefficients:\nIntercept           3.5679\nmcv                 0.6996\nalkphos             0.2249\nsgpt               -0.2766\nsgot                0.6109\ngammagt             0.6564\n\nResidual standard error: 3.1301 on 216 degrees of freedom\nMultiple R-squared: 0.1610,    Adjusted R-squared: 0.1455\nF-statistic: 10.3659 on 5 and 216 DF,  p-value: 1.067e-07"
  },
  {
    "objectID": "assignment.scicloj.html#build-final-models-for-evaluation",
    "href": "assignment.scicloj.html#build-final-models-for-evaluation",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.5 Build final models for evaluation",
    "text": "3.5 Build final models for evaluation\n\n3.5.1 Ridge\n\n(def eval-ridge\n  (evaluate-pipe\n    (-&gt;&gt; (extract-params models-ridge-val 3)                ;use best 3 lambdas\n         (map ridge-pipe-fn))\n    ds-split))\n\n\n(def models-ridge\n  (-&gt;&gt; (best-models eval-ridge)\n       reverse))\n\n\n\n3.5.2 Lasso\n\n(def eval-lasso\n  (evaluate-pipe\n    (-&gt;&gt; (extract-params models-lasso-val-2 3)              ;use best 3 lambdas\n         (map lasso-pipe-fn))\n    ds-split))\n\n\n(def models-lasso\n  (-&gt;&gt; (best-models eval-lasso)\n       reverse))\n\n\n\n3.5.3 Elastic net\n\n(def eval-enet\n  (evaluate-pipe\n    (-&gt;&gt; (extract-params models-enet-val 3)                 ;use best 3 lambda1s and lambda2s\n         (map elastic-net-pipe-fn))\n    ds-split))\n\n\n(def models-enet\n  (-&gt; (best-models eval-enet)\n      reverse))"
  },
  {
    "objectID": "assignment.scicloj.html#build-final-models-for-evaluation-1",
    "href": "assignment.scicloj.html#build-final-models-for-evaluation-1",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.6 Build final models for evaluation",
    "text": "3.6 Build final models for evaluation\n\n(def ds-ridge\n  (-&gt; (model-&gt;ds models-ridge 3)\n      (ds/rename-columns {:lambda :lambda2})\n      (ds/add-columns {:lambda1 0 :alpha 0})))\n\n\n(def ds-lasso\n  (-&gt; (model-&gt;ds models-lasso 3)\n      (ds/rename-columns {:lambda :lambda1})\n      (ds/add-columns {:lambda2 0 :alpha 1})))\n\n\n(def ds-elastic\n  (-&gt; (model-&gt;ds models-enet 3)\n      (ds/add-columns {:alpha (fn [ds]\n                                (dfn// (:lambda1 ds) (dfn/+ (:lambda1 ds) (:lambda2 ds))))})))\n\n\n(def col-order\n  [:model-type :compute-time-ns :alpha :lambda1 :lambda2 :adj-r2 :mae :rmse])"
  },
  {
    "objectID": "assignment.scicloj.html#final-comparisons",
    "href": "assignment.scicloj.html#final-comparisons",
    "title": "3  Clojure with Smile Algorithm",
    "section": "3.7 Final comparisons",
    "text": "3.7 Final comparisons\n\n(def top-scicloj\n  (-&gt; (model-&gt;ds (concat (ds/rows ds-ridge :as-maps) (ds/rows ds-lasso :as-maps) (ds/rows ds-elastic :as-maps)))\n      (ds/reorder-columns col-order)\n      (ds/order-by :adj-r2 :desc)))\n\n\ntop-scicloj\n\n_unnamed [9 8]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n:model-type\n:compute-time-ns\n:alpha\n:lambda1\n:lambda2\n:adj-r2\n:mae\n:rmse\n\n\n\n\n:smile.regression/ridge\n1365481\n0.00000000\n0.00000000\n196.78714859\n0.27138367\n2.19656820\n2.88244828\n\n\n:smile.regression/ridge\n1467237\n0.00000000\n0.00000000\n192.77108434\n0.27131830\n2.19552025\n2.88102276\n\n\n:smile.regression/ridge\n985196\n0.00000000\n0.00000000\n188.75502008\n0.27124938\n2.19445469\n2.87959384\n\n\n:smile.regression/lasso\n1664485\n1.00000000\n4.01606426\n0.00000000\n0.26141892\n2.73072125\n3.40320579\n\n\n:smile.regression/elastic-net\n1229910\n0.62499984\n100.00000000\n60.00004000\n0.26059398\n2.22496112\n2.92246056\n\n\n:smile.regression/elastic-net\n1176776\n0.65217371\n100.00000000\n53.33338000\n0.26021738\n2.22204401\n2.91718016\n\n\n:smile.regression/lasso\n1151502\n1.00000000\n8.03212851\n0.00000000\n0.25986847\n2.73272023\n3.40683967\n\n\n:smile.regression/elastic-net\n1237148\n0.68181793\n100.00000000\n46.66672000\n0.25984550\n2.21890789\n2.91171010\n\n\n:smile.regression/lasso\n1225671\n1.00000000\n12.04819277\n0.00000000\n0.25806319\n2.73472124\n3.41060937\n\n\n\n\n(comment\n  ((-&gt; models-ridge first :pipe-fn)\n   (merge (-&gt; models-ridge first :fit-ctx)\n          {:metamorph/data (ds/tail (:test (second ds-split)))\n           :metamorph/mode :transform})))\n\n\n\n\n\nsource: src/assignment/scicloj.clj"
  },
  {
    "objectID": "assignment.sklearn.html#build-pipelines",
    "href": "assignment.sklearn.html#build-pipelines",
    "title": "4  Clojure with Scikit Learn Algorithm",
    "section": "4.1 Build pipelines",
    "text": "4.1 Build pipelines\n\n(def pipeline-fn\n  (ml/pipeline\n    (mm/remove-column :selector)\n    (mm/std-scale regressors {})\n    (mm/set-inference-target response)))\n\nIn scikit.learn’s implementation of elastic net, it takes an alpha value, where \\(alpha = 0\\) is a ridge regression model, \\(alpha = 1\\) is a lasso regression model, and \\(0 &lt; alpha &lt; 1\\) is strictly an elastic net model that combines the loss functions of both ridge and lasso regression models at differing strengths. Closer to 0 means ridge regression loss function has a heavier consideration and alpha closer to 1 meaning the lasso loss function has a heavier consideration. (This is in general, there is an optional l1-ratio parameter that will change the above interpretation.)\n\n(defn sklearn-pipe-fn [params]\n  (ml/pipeline\n    pipeline-fn\n    {:metamorph/id :model}\n    (mm/model (merge {:model-type     :sklearn.regression/elastic-net\n                      :predict-proba? false}\n                     params))))"
  },
  {
    "objectID": "assignment.sklearn.html#partition-data",
    "href": "assignment.sklearn.html#partition-data",
    "title": "4  Clojure with Scikit Learn Algorithm",
    "section": "4.2 Partition data",
    "text": "4.2 Partition data\n\n(def ds-split                                               ;:split-names [:train-val :test]\n  (ds/split-&gt;seq liver-disease :kfold {:seed 123 :k 5 :ratio [0.8 0.2]}))\n\n\n(def train-val-splits\n  (ds/split-&gt;seq\n    (:train (first ds-split))\n    :kfold {:seed 123 :k 5}))"
  },
  {
    "objectID": "assignment.sklearn.html#evaluate-pipelines",
    "href": "assignment.sklearn.html#evaluate-pipelines",
    "title": "4  Clojure with Scikit Learn Algorithm",
    "section": "4.3 Evaluate pipelines",
    "text": "4.3 Evaluate pipelines\n\n(def sklearn-pipelines\n  (-&gt;&gt;\n    (ml/sobol-gridsearch {:alpha (ml/linear 0 1 250)})      ;doesnt like l1-ratio, why??\n    (map sklearn-pipe-fn)))\n\n\n(def evaluations-sklearn\n  (ml/evaluate-pipelines\n    sklearn-pipelines\n    train-val-splits\n    stats/omega-sq\n    :accuracy\n    {:other-metrices                   [{:name :mae :metric-fn ml/mae}\n                                        {:name :rmse :metric-fn ml/rmse}]\n     :return-best-pipeline-only        false\n     :return-best-crossvalidation-only true}))"
  },
  {
    "objectID": "assignment.sklearn.html#extract-models",
    "href": "assignment.sklearn.html#extract-models",
    "title": "4  Clojure with Scikit Learn Algorithm",
    "section": "4.4 Extract models",
    "text": "4.4 Extract models\n\n(def models-sklearn-vals\n  (-&gt;&gt; (best-models evaluations-sklearn)\n       reverse))\n\n\n(-&gt; models-sklearn-vals first :metric)\n\n\n0.3641237206779405\n\n\n(-&gt; models-sklearn-vals first :params)\n\n\n{:model-type :sklearn.regression/elastic-net,\n :predict-proba? false,\n :alpha 1.0}\n\n\n(-&gt; models-sklearn-vals first :fit-ctx :model :model-data :attributes :intercept_)\n\n\n3.567873303167421\n\n\n(-&gt; models-sklearn-vals first :fit-ctx :model :model-data :attributes :coef_)\n\n\n[0.23816559 0.         0.         0.16579044 0.25591104]\n\n\n(-&gt; (model-&gt;ds models-sklearn-vals 5)\n    (ds/reorder-columns col-order))\n\n_unnamed [5 7]:\n\n\n\n\n\n\n\n\n\n\n\n\n:model-type\n:compute-time-ns\n:alpha\n:adj-r2\n:mae\n:rmse\n:predict-proba?\n\n\n\n\n:sklearn.regression/elastic-net\n6331770\n1.00000000\n0.36412372\n2.77538202\n3.27570429\nfalse\n\n\n:sklearn.regression/elastic-net\n4065197\n0.99598394\n0.36409472\n2.77449564\n3.27469312\nfalse\n\n\n:sklearn.regression/elastic-net\n3652424\n0.99196787\n0.36406587\n2.77360744\n3.27368066\nfalse\n\n\n:sklearn.regression/elastic-net\n3245103\n0.98795181\n0.36403717\n2.77271740\n3.27266689\nfalse\n\n\n:sklearn.regression/elastic-net\n4218376\n0.98393574\n0.36400862\n2.77182552\n3.27165183\nfalse\n\n\n\n\n(-&gt; (model-&gt;ds models-sklearn-vals 5)\n    (ds/reorder-columns col-order)\n    (ds/order-by :adj-r2 :desc))\n\n_unnamed [5 7]:\n\n\n\n\n\n\n\n\n\n\n\n\n:model-type\n:compute-time-ns\n:alpha\n:adj-r2\n:mae\n:rmse\n:predict-proba?\n\n\n\n\n:sklearn.regression/elastic-net\n6331770\n1.00000000\n0.36412372\n2.77538202\n3.27570429\nfalse\n\n\n:sklearn.regression/elastic-net\n4065197\n0.99598394\n0.36409472\n2.77449564\n3.27469312\nfalse\n\n\n:sklearn.regression/elastic-net\n3652424\n0.99196787\n0.36406587\n2.77360744\n3.27368066\nfalse\n\n\n:sklearn.regression/elastic-net\n3245103\n0.98795181\n0.36403717\n2.77271740\n3.27266689\nfalse\n\n\n:sklearn.regression/elastic-net\n4218376\n0.98393574\n0.36400862\n2.77182552\n3.27165183\nfalse"
  },
  {
    "objectID": "assignment.sklearn.html#build-final-models-for-evaluation",
    "href": "assignment.sklearn.html#build-final-models-for-evaluation",
    "title": "4  Clojure with Scikit Learn Algorithm",
    "section": "4.5 Build final models for evaluation",
    "text": "4.5 Build final models for evaluation\n\n(def eval-sklearn\n  (evaluate-pipe\n    (-&gt;&gt; (extract-params models-sklearn-vals 5)             ;use best 3 alphas\n         (map sklearn-pipe-fn))\n    ds-split))\n\n\n(def models-sklearn\n  (-&gt;&gt; (best-models eval-sklearn)\n       reverse))\n\n\n(def top-sklearn\n  (-&gt; (model-&gt;ds models-sklearn 5)\n      (ds/reorder-columns col-order)\n      (ds/order-by :adj-r2 :desc)\n      (ds/drop-columns :predict-proba?)))\n\n\ntop-sklearn\n\n_unnamed [5 6]:\n\n\n\n\n\n\n\n\n\n\n\n:model-type\n:compute-time-ns\n:alpha\n:adj-r2\n:mae\n:rmse\n\n\n\n\n:sklearn.regression/elastic-net\n3434387\n0.98393574\n0.26161668\n2.30607905\n3.04582307\n\n\n:sklearn.regression/elastic-net\n3545495\n0.98795181\n0.26159480\n2.30679512\n3.04693277\n\n\n:sklearn.regression/elastic-net\n3777292\n0.99196787\n0.26157229\n2.30750974\n3.04804261\n\n\n:sklearn.regression/elastic-net\n5455140\n0.99598394\n0.26154913\n2.30822291\n3.04915259\n\n\n:sklearn.regression/elastic-net\n4709301\n1.00000000\n0.26152532\n2.30893464\n3.05026269\n\n\n\n\n\n\n\nsource: src/assignment/sklearn.clj"
  },
  {
    "objectID": "assignment.conclusion.html#ml-process",
    "href": "assignment.conclusion.html#ml-process",
    "title": "5  Conclusion",
    "section": "5.1 ML Process",
    "text": "5.1 ML Process\nThe process involved hyperparameter tuning for elastic net models built using Scikit Learn and Smile algorithms. The Machine learning process involved: 1) Partitioning the data into training, validation, and testing. 2) With the training and validation data, we tune the hyperparameters (lambdas in Smile and alpha in Scikit Learn). 3) Using the best hyperparameters, we build a final model with the training data as the combined training and validation tested on the testing data. The results of this process are in the two tables below:\n\ntop-scicloj\n\n_unnamed [9 8]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n:model-type\n:compute-time-ns\n:alpha\n:lambda1\n:lambda2\n:adj-r2\n:mae\n:rmse\n\n\n\n\n:smile.regression/ridge\n1365481\n0.00000000\n0.00000000\n196.78714859\n0.27138367\n2.19656820\n2.88244828\n\n\n:smile.regression/ridge\n1467237\n0.00000000\n0.00000000\n192.77108434\n0.27131830\n2.19552025\n2.88102276\n\n\n:smile.regression/ridge\n985196\n0.00000000\n0.00000000\n188.75502008\n0.27124938\n2.19445469\n2.87959384\n\n\n:smile.regression/lasso\n1664485\n1.00000000\n4.01606426\n0.00000000\n0.26141892\n2.73072125\n3.40320579\n\n\n:smile.regression/elastic-net\n1229910\n0.62499984\n100.00000000\n60.00004000\n0.26059398\n2.22496112\n2.92246056\n\n\n:smile.regression/elastic-net\n1176776\n0.65217371\n100.00000000\n53.33338000\n0.26021738\n2.22204401\n2.91718016\n\n\n:smile.regression/lasso\n1151502\n1.00000000\n8.03212851\n0.00000000\n0.25986847\n2.73272023\n3.40683967\n\n\n:smile.regression/elastic-net\n1237148\n0.68181793\n100.00000000\n46.66672000\n0.25984550\n2.21890789\n2.91171010\n\n\n:smile.regression/lasso\n1225671\n1.00000000\n12.04819277\n0.00000000\n0.25806319\n2.73472124\n3.41060937\n\n\n\n\ntop-sklearn\n\n_unnamed [5 6]:\n\n\n\n\n\n\n\n\n\n\n\n:model-type\n:compute-time-ns\n:alpha\n:adj-r2\n:mae\n:rmse\n\n\n\n\n:sklearn.regression/elastic-net\n3434387\n0.98393574\n0.26161668\n2.30607905\n3.04582307\n\n\n:sklearn.regression/elastic-net\n3545495\n0.98795181\n0.26159480\n2.30679512\n3.04693277\n\n\n:sklearn.regression/elastic-net\n3777292\n0.99196787\n0.26157229\n2.30750974\n3.04804261\n\n\n:sklearn.regression/elastic-net\n5455140\n0.99598394\n0.26154913\n2.30822291\n3.04915259\n\n\n:sklearn.regression/elastic-net\n4709301\n1.00000000\n0.26152532\n2.30893464\n3.05026269\n\n\n\n\n(double (/ (apply min (:compute-time-ns top-sklearn))\n           (apply max (:compute-time-ns top-scicloj))))\n\n\n2.063333103031869"
  },
  {
    "objectID": "assignment.conclusion.html#final-remarks",
    "href": "assignment.conclusion.html#final-remarks",
    "title": "5  Conclusion",
    "section": "5.2 Final Remarks",
    "text": "5.2 Final Remarks\nIn terms of the goodness-of-fit, both implementations perform similarly. The main difference is between compute times. Scikit Learn’s implementation takes 1.5 to 2.5 times longer than Smile’s (multiple runs). Choosing a best model, I’d pick Smile’s ridge regression with a lambda of 196.78714859. It has the benefit of fastest computational time and best Adjusted R\\(^2\\). The model coefficients are as follows:\n\n(-&gt; models-ridge first :summary)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -7.0208     -2.4216     -0.6945      1.7443     14.4280\n\nCoefficients:\nIntercept           3.5054\nmcv                 0.5182\nalkphos             0.0964\nsgpt                0.0410\nsgot                0.3163\ngammagt             0.3779\n\nResidual standard error: 3.1187 on 271 degrees of freedom\nMultiple R-squared: 0.1446,    Adjusted R-squared: 0.1319\nF-statistic: 11.4492 on 5 and 271 DF,  p-value: 1.334e-08\n\nOverall, the model is not a good fit. The model accounts for only 27.138% of the variance (note how the above summary is the model being evaluated on the training data, the tables above shows the metrics on the test data). Looking at the model we can say, that given every other variable remaining constant, for every one unit increase in :mcv, :drinks increases by 0.518; for every one unit increase in alkphos, :drinks increases by 0.096; for every one unit increase in :sgpt, :drinks increases by 0.041; for every one unit increase in :sgot, :drinks increases by 0.316; for every one unit increase in :gammagt, :drinks increases by 0.378.\n\n(comment\n  (-&gt; models-ridge first :summary .intercept)\n  (seq (-&gt; models-ridge first :summary .coefficients))\n  (-&gt; models-ridge first :summary .formula))\n\n\n\n\n\nsource: src/assignment/conclusion.clj"
  }
]